RAG-Powered Multi-Agent Q&A Assistant

This project implements a Retrieval-Augmented Generation (RAG) powered multi-agent Q&A assistant. The assistant retrieves relevant context from a document collection and generates natural-language answers using GPT-2. It includes an agentic workflow that routes specific queries (e.g., math or definitions) to appropriate tools.

Architecture
1. Document Retrieval (retrieval.py)
Document Ingestion: The system ingests text documents from a specified folder, processes them into semantic chunks, and indexes them using FAISS for efficient similarity search.

Chunking: Documents are split into chunks of sentences, ensuring semantic coherence while respecting a token limit.

Vector Store: A SentenceTransformer model encodes text into embeddings, and these embeddings are stored in a FAISS index for fast retrieval.

2. Agent Logic (agent.py)
QA Agent: The agent decides whether a query should be processed by the RAG pipeline or routed to a special tool (calculator or dictionary) based on the query's content.

Query Handling: If the query contains math or definition keywords, it’s sent to the appropriate tool. Otherwise, the system retrieves relevant context from the documents, generates an answer using GPT-2, and returns it.

Generation: Answers are generated by fine-tuning the GPT-2 model with the query and the retrieved context.

3. User Interface (app.py)
Streamlit UI: A minimal Streamlit web interface allows users to input questions and view answers. It also shows the agent’s workflow and the retrieved context.

Caching: Components like document ingestion and retrieval are cached for efficiency.

4. Text Generation and Definition Handling
GPT-2 is used to generate answers to queries, with custom prompts designed to generate concise responses.

For queries requiring definitions, GPT-2 generates simple, easy-to-understand explanations.

Key Design Choices
Modular Design:

The system is split into modular components: DocumentRetriever, QAAgent, and UI. This allows easy extension and maintenance.

Scalability:

The use of FAISS allows the retrieval system to scale to large document collections efficiently.

Query Routing:

Special handling for certain types of queries (e.g., math, definitions) ensures the system provides more accurate and appropriate responses.

UI & Workflow Transparency:

The Streamlit interface provides a user-friendly experience and transparency into the agent's decision-making and context retrieval process.

Running the Code
Prerequisites
Python 3.8+

Install required libraries:
pip install -r requirements.txt
Add the document(s) to the documents/ folder

Running the Application
Start the Streamlit app:
streamlit run app.py
Open the application in your browser at http://localhost:8501.

Folder Structure
/project-directory
│
├── /documents/             # Contains text files (e.g.,doc1.txt)
├── /app.py                 # Streamlit web UI
├── /retrieval.py           # Document retrieval and vector store
├── /agent.py               # QA agent with retrieval and generation logic
├── /requirements.txt       # Python dependencies
└── /README.md              # This file
